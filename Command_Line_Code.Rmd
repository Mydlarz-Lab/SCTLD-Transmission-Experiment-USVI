---
title: "Command Line Code"
author: "Kelsey Beavers"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

# FastP: Data Preprocessing
Each sample in the the transcriptome study is first put through a pre-processing step where adapters are trimmed and quality filtering is performed. This program used is called [FastP](https://github.com/OpenGene/fastp). This program was developed as an all-in-one pre-processing program for FastQ files with multi-threading support for high performance. The code in file fastp.sh is as follows: 
```{linux, eval=FALSE}
#!/bin/bash
PATH=$PATH:/opt/storage/opt_programs/fastp
for FILE in *_1.fq.gz; do
        echo ${FILE}
        SAMP=$(basename -s _1.fq.gz $FILE)
        echo $SAMP
nohup /opt/storage/opt_programs/fastp -i ${SAMP}_1.fq.gz -I ${SAMP}_2.fq.gz -o fastp/${SAMP}_fp_1.fq.gz -O fastp/${SAMP}_fp_2.fq.gz
done
```

# Transcriptome Assembly

## De novo transcriptome assembly 
### Step 1: P. strigosa metatranscriptome assembly
Five P.strigosa samples are randomly selected and their FastP adapter-trimmed and quality-filtered reads are used to generate a de novo reference transcriptome using [Trinity](https://github.com/trinityrnaseq/trinityrnaseq/wiki). This job was run on the Frontera supercomputer at the Texas Advanced Computing Center (TACC). The code in pstr_trinity.sh is as follows:
```{linux, eval=FALSE}
#!/bin/bash
module load tacc-singularity
PATH=$PATH:/scratch1/06825/tg861249
singularity exec -e /scratch1/06825/tg861249/trinityrnaseq.v2.14.0.simg Trinity --normalize_reads --seqType fq --grid_node_CPU 21 --grid_node_max_memory 300G --max_memory 300G --SS_lib_type FR --left P_c2_1.fq.gz,P_c5_1.fq.gz,P_c8_1.fq.gz,P_d3_1.fq.gz,P_d7_1.fq.gz --right P_c2_2.fq.gz,P_c5_2.fq.gz,P_c8_2.fq.gz,P_d3_2.fq.gz,P_d7_2.fq.gz --CPU 25
```
### Step 2: Obtain coral-only transcripts
Coral-only transcripts are extracted from the P.strigosa metatranscriptome following the protocol outlined by Dimos et al. (2022). The longest isoform sequence is obtained using the script in [Trinity](https://github.com/trinityrnaseq/trinityrnaseq/wiki). Usage is as follows:
```{linux, eval=FALSE}
trinityrnaseq-Trinity-v2.5.1/util/misc/get_longest_isoform_seq_per_trinity_gene.pl raw_transctipome.fa > processed_transcriptome.fa
```
A Master Coral database is created using [BLAST](https://www.ncbi.nlm.nih.gov/books/NBK279690/toc/?report=reader):
```{linux, eval=FALSE}
ncbi-blast-2.2.27+/bin/makeblastdb -in MasterCoral.fasta -parse_seqids -dbtype prot -out MasterCoral_db
```
The assembly is BLASTed against the Master Coral database:
```{linux, eval=FALSE}
ncbi-blast-2.2.27+/bin/blastn -query processed_transcriptome.fa -db MasterCoral_db -outfmt "6 qseqid evalue pident length" -max_target_seqs 1 -out [coral_only_sequences.txt]
```
Reads with less than 95% percent identity and shorter than 150 bp long are filtered out:
```{linux, eval=FALSE}
awk '{if ($3 > 95) print $1,$2,$4 }' coral_only_sequences.txt > contigs_percent_95.txt
awk '{if ($3 > 150) print $1}' contigs_percent_95.txt > contigs_percent_95_bp_150.txt
```
### Step 3: Extract coral-only transcripts from assembly
An index of the metatranscriptome assembly is created with [cdbfasta](https://github.com/gpertea/cdbfasta):
```{linux, eval=FALSE}
cdbfasta/cdbfasta raw_transctipome.fa
```
Coral-only contigs (contigs_percent_95_bp_150.txt) are matched to the metatranscriptome index
```{linux, eval=FALSE}
cat contigs_percent_95_bp_150.txt | cdbfasta/cdbyank raw_transctipome.fa.cidx > coral_only_transcriptome.fa
```
Extract the longest open reading frame from each contig and then generate its predicted peptide sequence using [TransDecoder](https://github.com/TransDecoder/TransDecoder/wiki):
```{linux, eval=FALSE}
TransDecoder.LongOrfs -t coral_only_transcriptome.fa
TransDecoder.Predict -t coral_only_transcriptome.fa
# Rename the resulting .pep file to end in .fa
```
Similar sequences are removed with [cd-hit](https://sites.google.com/view/cd-hit)
```{linux, eval=FALSE}
cd-hit -i coral_only_transcriptome_transdecoder.fa -o reference_proteome.fa
# This reference proteome is now ready for Orthofinder
```
### Step 4: Make an alignable transcriptome
Use the reference proteome to create an alignable transcriptome:
```{linux, eval=FALSE}
grep ">" reference_proteome.fa > proteome_names.txt
sed 's/.p/\t/' proteome_names.txt > proteome_names_format.txt
awk '{print $1}'  proteome_names_format.txt > contigs_to_extract.txt
sed 's/^.//g' contigs_to_extract.txt > contigs_list.txt

cat contigs_list.txt | cdbfasta/cdbyank raw_transctipome.fa.cidx > final_coral_reference_transcriptome.fa
```
### Step 5: Annotate the coral-only transcriptome
The latest [UniProt](https://www.uniprot.org/help/downloads) release (Reviewed) is downloaded and used to create a BLAST database:
```{linux, eval=FALSE}
wget https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz
gunzip uniprot_sprot.fasta.gz

ncbi-blast-2.2.27+/bin/makeblastdb -in uniprot_sprot.fasta -parse_seqids -dbtype nucl -out uniprot_db
```
Assembly is BLASTed against the database:
```{linux, eval=FALSE}
ncbi-blast-2.2.27+/bin/blastx -query final_coral_reference_transcriptome.fa -db uniprot_db -outfmt "6 sseqid qseqid evalue" -max_target_seqs 1 -out annotated_coral_reference_transcriptome.txt
```

## Genome-guided transcriptome assembly
### Step 1: Make index of genome
Genome index is created with [Bowtie2](https://bowtie-bio.sourceforge.net/bowtie2/index.shtml)
```{linux, eval=FALSE}
bowtie2-2.3.4-linux-x86_64/bowtie2-build Ofav_genome.fasta ofav_alignable.fasta
```
### Step 2: Align RNA-seq reads to genome
Each pair of reads is aligned to the genome index using [TopHat](https://ccb.jhu.edu/software/tophat/index.shtml)
```{linux, eval=FALSE}
tophat-2.1.1.Linux_x86_64/tophat2 --num-threads 12 ofav_alignable.fasta O_c1_1.fq.gz O_c1_2.fq.gz
tophat-2.1.1.Linux_x86_64/tophat2 --num-threads 12 ofav_alignable.fasta O_c2_1.fq.gz O_c2_2.fq.gz
tophat-2.1.1.Linux_x86_64/tophat2 --num-threads 12 ofav_alignable.fasta O_c3_1.fq.gz O_c3_2.fq.gz
tophat-2.1.1.Linux_x86_64/tophat2 --num-threads 12 ofav_alignable.fasta O_c6_1.fq.gz O_c6_2.fq.gz
tophat-2.1.1.Linux_x86_64/tophat2 --num-threads 12 ofav_alignable.fasta O_c7_1.fq.gz O_c7_2.fq.gz
tophat-2.1.1.Linux_x86_64/tophat2 --num-threads 12 ofav_alignable.fasta O_c8_1.fq.gz O_c8_2.fq.gz
tophat-2.1.1.Linux_x86_64/tophat2 --num-threads 12 ofav_alignable.fasta O_d1_1.fq.gz O_d1_2.fq.gz
tophat-2.1.1.Linux_x86_64/tophat2 --num-threads 12 ofav_alignable.fasta O_d2_1.fq.gz O_d2_2.fq.gz
tophat-2.1.1.Linux_x86_64/tophat2 --num-threads 12 ofav_alignable.fasta O_d3_1.fq.gz O_d3_2.fq.gz
tophat-2.1.1.Linux_x86_64/tophat2 --num-threads 12 ofav_alignable.fasta O_d6_1.fq.gz O_d6_2.fq.gz
tophat-2.1.1.Linux_x86_64/tophat2 --num-threads 12 ofav_alignable.fasta O_d7_1.fq.gz O_d7_2.fq.gz
tophat-2.1.1.Linux_x86_64/tophat2 --num-threads 12 ofav_alignable.fasta O_d8_1.fq.gz O_d8_2.fq.gz
```
### Step 3: Merge and Sort 
Resulting accepted_hits.bam files are renamed by their sample name and moved into the same directory. Then they are merged and sorted using [Samtools](https://www.htslib.org)
```{linux, eval=FALSE}
samtools merge -o finalBamFile.bam *.bam
samtools sort finalBamFile.bam -o Sorted.bam
```
### Step 4: Genome-guided assembly 
The Sorted.bam file is used by [Trinity](https://github.com/trinityrnaseq/trinityrnaseq/wiki) to generate a genome-guided assembly. This job was run on the Lonestar5 supercomputer at the Texas Advanced Computing Center (TACC). The code in file oann_gg_trinity.sh is as follows: 
```{linux, eval=FALSE}
module load tacc-singularity
PATH=$PATH:/scratch/06825/tg861249/oann_containerized_run
singularity exec -e trinityrnaseq.v2.11.0.simg Trinity --genome_guided_max_intron 10000 --normalize_reads --grid_node_CPU 32 --grid_node_max_memory 500G --max_memory 500G --SS_lib_type FR --genome_guided_bam Sorted.bam --CPU 32
```

# BBSplit: Sort coral and Symbiodiniaceae reads
Our eukaryotic reads contain sequences that originate from both the coral host species as well as their intracellular Symbiodiniaceae. There are four predominant Symbiodiniaceae genera that form symbioses with the corals in our study, so we will use [BBMap](https://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/bbmap-guide/) to map reads to the coral host transcriptome, as well as Symbiodinium, Breviolum, Cladocopium, and Durusdinium transcriptomes, prior to read quantification. BBMap is splice-aware global aligner for DNA and RNA sequencing reads, and BBsplit uses BBMap to map reads to multiple transcriptomes at once and determines which transcriptome each reads matches to best. Usage is as follows:
```{linux, eval=FALSE}
```{linux, eval=FALSE}
#!/bin/bash
PATH=$PATH:/opt/storage/opt_programs/bbmap/
PATH=$PATH:/opt/storage/anaconda3/bin/java
DIR=/opt/storage/storage/transcriptomes
for FILE in *_fp_1.fq.gz; do
        echo ${FILE}
        SAMP=$(basename -s _fp_1.fq.gz $FILE)
        echo $SAMP
        
nohup /opt/storage/anaconda3/bin/java -ea -Xmx10g -cp /opt/storage/opt_programs/bbmap/current/ align2.BBSplitter in1=${SAMP}_fp_1.fq.gz in2=${SAMP}_fp_2.fq.gz ref=${DIR}/coral-host/[coral_reference_transcriptome.fa],${DIR}/symbiont/CladeD_transcriptome.fasta,${DIR}/symbiont/davies_cladeC_feb_transcriptome.fasta,${DIR}/symbiont/Symbiont_clade_B_reference.fasta,${DIR}/symbiont/Symbiont_type_A.fasta basename=${SAMP}_%.fq.gz refstats=${SAMP}_stats.txt outu1=${SAMP}_bboutu_1.fq.gz outu2=${SAMP}_bboutu_2.fq.gz
done 
```
This will create one output for each file. The text file for ref stats can be downloaded and placed into BBSplit Statistics output file for all species. It will be used at bottom for Summary of Genus Averages. 

From here, each file will need to be reformatted with BBMap program to split back into two separate fastq files for forward and reverse reads. 
```{linux, eval=FALSE}
#!/bin/bash
PATH=$PATH:/opt/storage/opt_programs/bbmap/
PATH=$PATH:/opt/storage/anaconda3/bin/java
for FILE in *_host.fq.gz; do
        echo ${FILE}
        SAMP=$(basename -s .fq.gz $FILE)
        echo $SAMP
nohup /opt/storage/anaconda3/bin/java -ea -Xmx10g  -cp  /opt/storage/opt_programs/bbmap/current/ jgi.ReformatReads in=${SAMP}.fq out1=${SAMP}_1.fq out2=${SAMP}_2.fq
done
```

# Salmon: Read Quantification
[Salmon](https://salmon.readthedocs.io/en/latest/salmon.html#) is a tool built for transcript quantification. It uses two phases; indexing and quantification, to map samples. The first step, indexing, is independent of the reads and requires a reference transcript to build an index. Code for that is as follows: 

## Index Building for Host Transcriptomes 
For the host indexes, we can keep kmer values at a standard as we are confidant in the transcriptomes we have just built and the quality of the transcriptome. 
```{linux, eval=FALSE}
#	working directory: /opt/storage/storage/symbiont_salmon_indices_and_loops/indexes_k23
# k value dropped from 31 standard to 23 because of quality of fasta index file 

PATH=$PATH:/opt/storage/opt_programs/Salmon/salmon-0.12.0_linux_x86_64/bin/

# Cnat index 
nohup /opt/storage/opt_programs/Salmon/salmon-0.12.0_linux_x86_64/bin/salmon index -t ../../transcriptomes/coral-host/-i Cnat_index 

# Mcav index 
nohup /opt/storage/opt_programs/Salmon/salmon-0.12.0_linux_x86_64/bin/salmon index -t ../../transcriptomes/coral-host/-i Mcav_index 

# Oanu index 
nohup /opt/storage/opt_programs/Salmon/salmon-0.12.0_linux_x86_64/bin/salmon index -t ../../transcriptomes/coral-host/-i Oanu_index 

# Past index 
nohup /opt/storage/opt_programs/Salmon/salmon-0.12.0_linux_x86_64/bin/salmon index -t ../../transcriptomes/coral-host/-i Cnat_index

```

## Index Building for Symbiont Transcriptomes 
For the symbiont indexes, we can drop kmer values at a standard in order to get the best quality of reads.  
```{linux, eval=FALSE}
#	working directory: /opt/storage/storage/symbiont_salmon_indices_and_loops/indexes_k23
# k value dropped from 31 standard to 23 because of quality of fasta index file 
PATH=$PATH:/opt/storage/opt_programs/Salmon/salmon-0.12.0_linux_x86_64/bin/
# Durusdinium index
nohup /opt/storage/opt_programs/Salmon/salmon-0.12.0_linux_x86_64/bin/salmon index -t ../../transcriptomes/CladeD_transcriptome.fasta -i Clade_D_index -k 23
# Cladocopium index
/opt/storage/opt_programs/Salmon/salmon-0.12.0_linux_x86_64/bin/salmon index -t ../../transcriptomes/davies_cladeC_feb_transcriptome.fasta -i Clade_C_index -k 23
# Breviolum index
nohup /opt/storage/opt_programs/Salmon/salmon-0.12.0_linux_x86_64/bin/salmon index -t ../../transcriptomes/Symbiont_clade_B_reference.fasta -i Clade_B_index -k 23
# Symbiodinium index
nohup /opt/storage/opt_programs/Salmon/salmon-0.12.0_linux_x86_64/bin/salmon index -t ../../transcriptomes/Symbiont_type_A.fasta -i Clade_A_index -k 23
```

## Mapping Reads: Use Salmon for quasi-mapping results 
[Salmon](https://salmon.readthedocs.io/en/latest/salmon.html#) is a tool built for transcript quantification. It uses two phases; indexing and quantification, to map samples. The second phase: quantification, using quasi-mapping program to map samples to index. Quasi-mapping assumes a generally small file and increased number of repeats in reference sequences. It also takes into account splicing because a transcriptome is assumed to be used for the index. 

```{linux, eval=FALSE}
#  move fastp filtered files into you salmon directory
#  since we'll be running the same command on each sample, the simplest way to automate this process is with a simple shell script
#  the script loops through each sample and invokes salmon using fairly barebone options
# -i  tells salmon where to find the index
# -l A tells salmon that it should automatically determine the library type of the sequencing reads (stranded vs. unstranded etc.)
# -1 and -2 tell salmon where to find the left and right reads for this sample (notice, salmon will accept gzipped FASTQ files directly)
# -p 8 tells salmon to make use of 8 threads
# -o specifies the directory where salmon's quantification results should be written
# working directory: /opt/storage/storage/SCTLD/cnat/salmon

PATH=$PATH:/opt/storage/opt_programs/Salmon/salmon-0.12.0_linux_x86_64/bin/
./salmon_loop.sh
```

salmon_loop.sh file: 
```{linux, eval=FALSE}
# Loop for Cnat
#!/bin/bash
PATH=$PATH:/scratch1/06825/tg861249/salmon-1.5.2_linux_x86_64/bin
for FILE in *_1.fq; do
        echo ${FILE}
        SAMP=$(basename -s _1.fq $FILE)
        echo $SAMP
        DIR=/scratch1/06825/tg861249/salmon_indeces

/scratch1/06825/tg861249/salmon-1.5.2_linux_x86_64/bin/salmon quant -i ${DIR}/Cnat_index -l A \
				-1 ${SAMP}_1.fq \
				-2 ${SAMP}_2.fq \
				-p 8 --validateMappings -o quants/${SAMP}_quant
done

# Loop for Mcav
#!/bin/bash
PATH=$PATH:/scratch1/06825/tg861249/salmon-1.5.2_linux_x86_64/bin
for FILE in *_1.fq; do
        echo ${FILE}
        SAMP=$(basename -s _1.fq $FILE)
        echo $SAMP
        DIR=/scratch1/06825/tg861249/salmon_indeces

/scratch1/06825/tg861249/salmon-1.5.2_linux_x86_64/bin/salmon quant -i ${DIR}/Mcav_index -l A \
				-1 ${SAMP}_1.fq \
				-2 ${SAMP}_2.fq \
				-p 8 --validateMappings -o quants/${SAMP}_quant
done

# Loop for Oanu
#!/bin/bash
PATH=$PATH:/scratch1/06825/tg861249/salmon-1.5.2_linux_x86_64/bin
for FILE in *_1.fq; do
        echo ${FILE}
        SAMP=$(basename -s _1.fq $FILE)
        echo $SAMP
        DIR=/scratch1/06825/tg861249/salmon_indeces
        
/scratch1/06825/tg861249/salmon-1.5.2_linux_x86_64/bin/salmon quant -i ${DIR}/Oann_index -l A \
				-1 ${SAMP}_1.fq \
				-2 ${SAMP}_2.fq \
				-p 8 --validateMappings -o quants/${SAMP}_quant
done

# Loop for Past
#!/bin/bash
PATH=$PATH:/scratch1/06825/tg861249/salmon-1.5.2_linux_x86_64/bin
for FILE in *_1.fq; do
        echo ${FILE}
        SAMP=$(basename -s _1.fq $FILE)
        echo $SAMP
        DIR=/scratch1/06825/tg861249/salmon_indeces
        
/scratch1/06825/tg861249/salmon-1.5.2_linux_x86_64/bin/salmon quant -i ${DIR}/Past_index -l A \
				-1 ${SAMP}_1.fq \
				-2 ${SAMP}_2.fq \
				-p 8 --validateMappings -o quants/${SAMP}_quant
done

# Loop for Pstr
#!/bin/bash
PATH=$PATH:/scratch1/06825/tg861249/salmon-1.5.2_linux_x86_64/bin

for FILE in *_1.fq; do
        echo ${FILE}
        SAMP=$(basename -s _1.fq $FILE)
        echo $SAMP
        DIR=/scratch1/06825/tg861249/salmon_indeces

/scratch1/06825/tg861249/salmon-1.5.2_linux_x86_64/bin/salmon quant -i ${DIR}/Pstr_index -l A \
				-1 ${SAMP}_1.fq \
				-2 ${SAMP}_2.fq \
				-p 8 --validateMappings -o quants/${SAMP}_quant
done

# Loop for Symbiodinium
#!/bin/bash
PATH=$PATH:/scratch1/06825/tg861249/salmon-1.5.2_linux_x86_64/bin
for FILE in *_1.fq; do
        echo ${FILE}
        SAMP=$(basename -s _1.fq $FILE)
        echo $SAMP
        DIR=/scratch1/06825/tg861249/salmon_indeces

/scratch1/06825/tg861249/salmon-1.5.2_linux_x86_64/bin/salmon quant -i ${DIR}/cladeA_index -l A \
				-1 ${SAMP}_1.fq \
				-2 ${SAMP}_2.fq \
				-p 8 --validateMappings -o quants/${SAMP}_quant
done

# Loop for Breviolum
#!/bin/bash
PATH=$PATH:/scratch1/06825/tg861249/salmon-1.5.2_linux_x86_64/bin
for FILE in *_1.fq; do
        echo ${FILE}
        SAMP=$(basename -s _1.fq $FILE)
        echo $SAMP
        DIR=/scratch1/06825/tg861249/salmon_indeces

/scratch1/06825/tg861249/salmon-1.5.2_linux_x86_64/bin/salmon quant -i ${DIR}/cladeB_index -l A \
				-1 ${SAMP}_1.fq \
				-2 ${SAMP}_2.fq \
				-p 8 --validateMappings -o quants/${SAMP}_quant
done

# Loop for Cladocopium
#!/bin/bash
PATH=$PATH:/scratch1/06825/tg861249/salmon-1.5.2_linux_x86_64/bin
for FILE in *_1.fq; do
        echo ${FILE}
        SAMP=$(basename -s _1.fq $FILE)
        echo $SAMP
        DIR=/scratch1/06825/tg861249/salmon_indeces

/scratch1/06825/tg861249/salmon-1.5.2_linux_x86_64/bin/salmon quant -i ${DIR}/cladeC_index -l A \
				-1 ${SAMP}_1.fq \
				-2 ${SAMP}_2.fq \
				-p 8 --validateMappings -o quants/${SAMP}_quant
done

# Loop for Durusdinium
#!/bin/bash
PATH=$PATH:/scratch1/06825/tg861249/salmon-1.5.2_linux_x86_64/bin
for FILE in *_1.fq; do
        echo ${FILE}
        SAMP=$(basename -s _1.fq $FILE)
        echo $SAMP
        DIR=/scratch1/06825/tg861249/salmon_indeces

/scratch1/06825/tg861249/salmon-1.5.2_linux_x86_64/bin/salmon quant -i ${DIR}/cladeD_index -l A \
				-1 ${SAMP}_1.fq \
				-2 ${SAMP}_2.fq \
				-p 8 --validateMappings -o quants/${SAMP}_quant
done
```

# Orthofinder: Obtaining single-copy orthologs
[OrthoFinder](https://github.com/davidemms/OrthoFinder) can be used to find single-copy orthologs between a set of protein sequence files (one per species) in FASTA format. We will use this program to identify single-copy orthologs between the five coral species in our study and the four Symbiodiniaceae genera in our study. 
First we use [TransDecoder](https://github.com/TransDecoder/TransDecoder/wiki) to generate predicted proteomes from our transcriptomes (if not already done during transcriptome assembly). Usage is as follows:
```{linux, eval=FALSE}
TransDecoder.LongOrfs -t coral_only_transcriptome.fa
TransDecoder.Predict -t coral_only_transcriptome.fa
# Rename the resulting .pep file to end in .fa
```
Similar sequences are removed with [cd-hit](https://sites.google.com/view/cd-hit)
```{linux, eval=FALSE}
cd-hit -i coral_only_transcriptome_transdecoder.fa -o reference_proteome.fa
# This reference proteome is now ready for Orthofinder
```
Move all proteomes of interest into the same directory. Then run OrthoFinder as follows:
```{linux, eval=FALSE}
python2.7 OrthoFinder-2.5.2/orthofinder.py -f . -t 4
```
This will generate a directory called "OrthoFinder/Results\_[date]". Move the "Orthogroups" and the "Comparative_Genomics_Statistics" subdirectories onto your local machine.

## Annotating the orthologs
From the /Orthogroups directory, use "Orthogroups_SingleCopyOrthologues.txt" and "Orthogroups.tsv" to grab the sequence names of the single-copy orthologs from each species.
```{r}
setwd("~/OneDrive - University of Texas at Arlington/Dissertation/SCTLD/Files_for_R/January_2022/SCTLD/host/all/Orthofinder_cdhit/Orthogroups/")
Orthogroups_SingleCopyOrthologues <- read.csv("Orthogroups_SingleCopyOrthologues.txt",header = FALSE)
colnames(Orthogroups_SingleCopyOrthologues) <- c("Orthogroup")
Orthogroups <- read.table(file = 'Orthogroups.tsv', sep = '\t', header = TRUE)
Orthogroup_Sequences <- merge(Orthogroups_SingleCopyOrthologues,Orthogroups,by="Orthogroup")
write.csv(Orthogroup_Sequences, file="Orthogroup_Sequences.csv",quote = FALSE,row.names = FALSE)

# We also grab the sequence names from P. astreoides (the species with highest quality assembly) and will use these for annotation 
past_orthologs <- Orthogroup_Sequences[c(5)]
names(past_orthologs) <- NULL
write.csv(past_orthologs, file="past_SC_ortholog_sequence_names.txt",quote = FALSE,row.names = FALSE)
```
Make a new directory in "OrthoFinder/" called "Annotating_Orthogroups" and move "past_SC_ortholog_sequences.csv" here along with the P.astreoides proteome from the previous step. We will use [cdbfasta](https://github.com/gpertea/cdbfasta) to pull the single-copy ortholog sequences out of the P.astreoides reference proteome. 
```{linux, eval=FALSE}
DIR=/opt/storage/storage/SCTLD/host_orthofinder/OrthoFinder/OrthoFinder/Annotating_Orthogroups

# Make an index of the P.astreoides reference proteome
cdbfasta/cdbfasta past_ref_proteome.fa

cat past_SC_ortholog_sequence_names.txt | cdbfasta/cdbyank past_ref_proteome.fa.cidx > past_sc_orthologs.fa

# Annotate with BLASTp
ncbi-blast-2.2.27+/bin/blastp -query past_sc_orthologs.fa -db uniprot_db -outfmt "6 sseqid qseqid evalue" -max_target_seqs 1 -out past_sc_orthologs_annotated.txt
```
Save "past_sc_orthologs_annotated.txt" and "past_sc_orthologs.fa" to your computer. We will use these files in our Coral EVE analysis. 
This process is analagous in the Symbiodinaiceae genera using the Durusdinium reference proteome as the reference for annotation.


